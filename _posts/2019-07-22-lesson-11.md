---
layout: post
published: false
title: Lesson 11
---
## Topic Modeling!

This is a relatively straight forward little discourse, given that it's more of a breakdown of what occurred than heavily annotated code. 

I was unable to run the entire Dispatch, so made due with a few year by year tests and a whole bunch of different numbers of topics.

LDA Topic modeling functions incredibly intuitively, and the "ideal" topic numbers make themselves rather clear.

For example, running the dataset with three topics resulted (obviously) in very little useful information, and running it with 100 topics had an obscene amount of overlap to the point that there was little one could conclude, other than that too many topics obscures the issue.

That said, the war bonds/south running out of money theme remained noteworthy, as is also noted in "Mining the Dispatch"

To use the same vocabulary utilized by Rob Nelson - both extremely high and extremely low numbers of topics generate topics that lack coherence.  They get rather silly.  This makes sense - in the most reductive fashion, we're telling the algorithm to bag up groups of words for us.  With just three topics, it's getting them from all over, and with 100, it's searching desperately for somethign else that could be coherent.  This leads to a number of topics (as noted on one of Nelson's excluded topics) that are purely geography, and some that are just jumbles.